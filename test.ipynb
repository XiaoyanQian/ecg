{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CLIP Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_grad_cam import GradCAM, GradCAMPlusPlus\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "\n",
    "# 参数类，直接用于设置运行所需的参数\n",
    "class Args:\n",
    "    image_path = \"../examples/both.png\"  # 替换为你的图像路径\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    method = \"gradcam\"\n",
    "    output_path = \"../output/CLIP_output.jpg\"\n",
    "\n",
    "\n",
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self, labels):\n",
    "        super().__init__()\n",
    "        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        self.labels = labels\n",
    "\n",
    "    def forward(self, x):\n",
    "        text_inputs = self.processor(text=self.labels, return_tensors=\"pt\", padding=True)\n",
    "        outputs = self.clip(pixel_values=x, input_ids=text_inputs['input_ids'].to(self.clip.device),\n",
    "                            attention_mask=text_inputs['attention_mask'].to(self.clip.device))\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        probs = logits_per_image.softmax(dim=1)\n",
    "        for label, prob in zip(self.labels, probs[0]):\n",
    "            print(f\"{label}: {prob:.4f}\")\n",
    "        return probs\n",
    "\n",
    "\n",
    "def reshape_transform(tensor, height=16, width=16):\n",
    "    \"\"\"\n",
    "    Reshape the Vision Transformer output to match the necessary shape for Grad-CAM.\n",
    "    \"\"\"\n",
    "    tensor = tensor[:, 1:, :].reshape(tensor.size(0), height, width, tensor.size(2))\n",
    "    return tensor.permute(0, 3, 1, 2)  # Rearrange to (batch, channels, height, width)\n",
    "\n",
    "\n",
    "def run_grad_cam(args):\n",
    "    # Default labels\n",
    "    labels = [\"a cat\", \"a dog\", \"a car\", \"a person\", \"a shoe\"]\n",
    "\n",
    "    # Load model and set to evaluation mode\n",
    "    model = ImageClassifier(labels).to(args.device).eval()\n",
    "\n",
    "    # Select target layers (ViT's LayerNorm)\n",
    "    target_layers = [model.clip.vision_model.encoder.layers[-1].layer_norm1]\n",
    "\n",
    "    # Read and preprocess the image\n",
    "    rgb_img = cv2.imread(args.image_path, 1)[:, :, ::-1]\n",
    "    rgb_img = cv2.resize(rgb_img, (224, 224))\n",
    "    rgb_img = np.float32(rgb_img) / 255\n",
    "    input_tensor = preprocess_image(rgb_img, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]).to(args.device)\n",
    "\n",
    "    # Initialize Grad-CAM with reshape_transform\n",
    "    cam_method = GradCAM if args.method == \"gradcam\" else GradCAMPlusPlus\n",
    "    cam = cam_method(model=model, target_layers=target_layers, reshape_transform=reshape_transform)\n",
    "\n",
    "    # Use the highest scoring category as default target\n",
    "    targets = None\n",
    "    targets = [ClassifierOutputTarget(1)]\n",
    "\n",
    "    # Generate CAM\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "    grayscale_cam = grayscale_cam[0, :]  # Take the first (and only) image in the batch\n",
    "\n",
    "    # Overlay CAM on the image\n",
    "    cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "\n",
    "    # Save and display the result\n",
    "    os.makedirs(os.path.dirname(args.output_path), exist_ok=True)  # 确保输出目录存在\n",
    "    cv2.imwrite(args.output_path, cam_image)\n",
    "    print(f\"Grad-CAM result saved to {args.output_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 直接使用手动设置的参数\n",
    "    args = Args()\n",
    "    run_grad_cam(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MERL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ecg_model': 'vit_tiny', 'num_leads': 12, 'text_model': 'ncbi/MedCPT-Query-Encoder', 'free_layers': 6, 'feature_dim': 768, 'projection_head': {'mlp_hidden_size': 256, 'projection_size': 256}}\n",
      "ECG signal shape: torch.Size([1, 12, 5000])\n",
      "ECG signal shape before Grad-CAM: torch.Size([1, 12, 1, 5000])\n",
      "Processing Lead 1\n",
      "Input signal shape for Lead 1: torch.Size([1, 12, 1, 5000])\n",
      "Input shape before squeeze: torch.Size([1, 12, 1, 5000])\n",
      "Input shape after squeeze: torch.Size([1, 12, 5000])\n",
      "Default text input: ['LV Impaired']\n",
      "Reshaped tensor shape: torch.Size([1, 100, 1, 192])\n",
      "Output shape: torch.Size([1, 1, 256])\n",
      "Reshaped tensor shape: torch.Size([1, 100, 1, 192])\n",
      "Processing Lead 2\n",
      "Input signal shape for Lead 2: torch.Size([1, 12, 1, 5000])\n",
      "Input shape before squeeze: torch.Size([1, 12, 1, 5000])\n",
      "Input shape after squeeze: torch.Size([1, 12, 5000])\n",
      "Default text input: ['LV Impaired']\n",
      "Reshaped tensor shape: torch.Size([1, 100, 1, 192])\n",
      "Output shape: torch.Size([1, 1, 256])\n",
      "Reshaped tensor shape: torch.Size([1, 100, 1, 192])\n",
      "Processing Lead 3\n",
      "Input signal shape for Lead 3: torch.Size([1, 12, 1, 5000])\n",
      "Input shape before squeeze: torch.Size([1, 12, 1, 5000])\n",
      "Input shape after squeeze: torch.Size([1, 12, 5000])\n",
      "Default text input: ['LV Impaired']\n",
      "Reshaped tensor shape: torch.Size([1, 100, 1, 192])\n",
      "Output shape: torch.Size([1, 1, 256])\n",
      "Reshaped tensor shape: torch.Size([1, 100, 1, 192])\n",
      "Processing Lead 4\n",
      "Input signal shape for Lead 4: torch.Size([1, 12, 1, 5000])\n",
      "Input shape before squeeze: torch.Size([1, 12, 1, 5000])\n",
      "Input shape after squeeze: torch.Size([1, 12, 5000])\n",
      "Default text input: ['LV Impaired']\n",
      "Reshaped tensor shape: torch.Size([1, 100, 1, 192])\n",
      "Output shape: torch.Size([1, 1, 256])\n",
      "Reshaped tensor shape: torch.Size([1, 100, 1, 192])\n",
      "Processing Lead 5\n",
      "Input signal shape for Lead 5: torch.Size([1, 12, 1, 5000])\n",
      "Input shape before squeeze: torch.Size([1, 12, 1, 5000])\n",
      "Input shape after squeeze: torch.Size([1, 12, 5000])\n",
      "Default text input: ['LV Impaired']\n",
      "Reshaped tensor shape: torch.Size([1, 100, 1, 192])\n",
      "Output shape: torch.Size([1, 1, 256])\n",
      "Reshaped tensor shape: torch.Size([1, 100, 1, 192])\n",
      "Processing Lead 6\n",
      "Input signal shape for Lead 6: torch.Size([1, 12, 1, 5000])\n",
      "Input shape before squeeze: torch.Size([1, 12, 1, 5000])\n",
      "Input shape after squeeze: torch.Size([1, 12, 5000])\n",
      "Default text input: ['LV Impaired']\n",
      "Reshaped tensor shape: torch.Size([1, 100, 1, 192])\n",
      "Output shape: torch.Size([1, 1, 256])\n",
      "Reshaped tensor shape: torch.Size([1, 100, 1, 192])\n",
      "Processing Lead 7\n",
      "Input signal shape for Lead 7: torch.Size([1, 12, 1, 5000])\n",
      "Input shape before squeeze: torch.Size([1, 12, 1, 5000])\n",
      "Input shape after squeeze: torch.Size([1, 12, 5000])\n",
      "Default text input: ['LV Impaired']\n",
      "Reshaped tensor shape: torch.Size([1, 100, 1, 192])\n",
      "Output shape: torch.Size([1, 1, 256])\n",
      "Reshaped tensor shape: torch.Size([1, 100, 1, 192])\n",
      "Processing Lead 8\n",
      "Input signal shape for Lead 8: torch.Size([1, 12, 1, 5000])\n",
      "Input shape before squeeze: torch.Size([1, 12, 1, 5000])\n",
      "Input shape after squeeze: torch.Size([1, 12, 5000])\n",
      "Default text input: ['LV Impaired']\n",
      "Reshaped tensor shape: torch.Size([1, 100, 1, 192])\n",
      "Output shape: torch.Size([1, 1, 256])\n",
      "Reshaped tensor shape: torch.Size([1, 100, 1, 192])\n",
      "Processing Lead 9\n",
      "Input signal shape for Lead 9: torch.Size([1, 12, 1, 5000])\n",
      "Input shape before squeeze: torch.Size([1, 12, 1, 5000])\n",
      "Input shape after squeeze: torch.Size([1, 12, 5000])\n",
      "Default text input: ['LV Impaired']\n",
      "Reshaped tensor shape: torch.Size([1, 100, 1, 192])\n",
      "Output shape: torch.Size([1, 1, 256])\n",
      "Reshaped tensor shape: torch.Size([1, 100, 1, 192])\n",
      "Processing Lead 10\n",
      "Input signal shape for Lead 10: torch.Size([1, 12, 1, 5000])\n",
      "Input shape before squeeze: torch.Size([1, 12, 1, 5000])\n",
      "Input shape after squeeze: torch.Size([1, 12, 5000])\n",
      "Default text input: ['LV Impaired']\n",
      "Reshaped tensor shape: torch.Size([1, 100, 1, 192])\n",
      "Output shape: torch.Size([1, 1, 256])\n",
      "Reshaped tensor shape: torch.Size([1, 100, 1, 192])\n",
      "Processing Lead 11\n",
      "Input signal shape for Lead 11: torch.Size([1, 12, 1, 5000])\n",
      "Input shape before squeeze: torch.Size([1, 12, 1, 5000])\n",
      "Input shape after squeeze: torch.Size([1, 12, 5000])\n",
      "Default text input: ['LV Impaired']\n",
      "Reshaped tensor shape: torch.Size([1, 100, 1, 192])\n",
      "Output shape: torch.Size([1, 1, 256])\n",
      "Reshaped tensor shape: torch.Size([1, 100, 1, 192])\n",
      "Processing Lead 12\n",
      "Input signal shape for Lead 12: torch.Size([1, 12, 1, 5000])\n",
      "Input shape before squeeze: torch.Size([1, 12, 1, 5000])\n",
      "Input shape after squeeze: torch.Size([1, 12, 5000])\n",
      "Default text input: ['LV Impaired']\n",
      "Reshaped tensor shape: torch.Size([1, 100, 1, 192])\n",
      "Output shape: torch.Size([1, 1, 256])\n",
      "Reshaped tensor shape: torch.Size([1, 100, 1, 192])\n",
      "Generated Grad-CAM for 12 leads.\n",
      "ECG image resolution: 1562 x 2205\n",
      "Scaled lead coordinates: {'I': (428, 566, 169, 1043), 'II': (585, 680, 169, 1043), 'III': (708, 828, 169, 1043), 'aVR': (829, 998, 169, 1043), 'aVL': (1007, 1148, 169, 1043), 'aVF': (1158, 1311, 169, 1043), 'V1': (428, 566, 1065, 1957), 'V2': (585, 680, 1065, 1957), 'V3': (708, 828, 1065, 1957), 'V4': (829, 998, 1065, 1957), 'V5': (1007, 1148, 1065, 1957), 'V6': (1158, 1311, 1065, 1957)}\n",
      "Processing Lead: I\n",
      "Grad-CAM shape for I: (1, 5000)\n",
      "Lead I: y=(428, 566), x=(169, 1043)\n",
      "Grad-CAM shape: (1, 5000)\n",
      "Saved Grad-CAM for I\n",
      "Processing Lead: II\n",
      "Grad-CAM shape for II: (1, 5000)\n",
      "Lead II: y=(585, 680), x=(169, 1043)\n",
      "Grad-CAM shape: (1, 5000)\n",
      "Saved Grad-CAM for II\n",
      "Processing Lead: III\n",
      "Grad-CAM shape for III: (1, 5000)\n",
      "Lead III: y=(708, 828), x=(169, 1043)\n",
      "Grad-CAM shape: (1, 5000)\n",
      "Saved Grad-CAM for III\n",
      "Processing Lead: aVR\n",
      "Grad-CAM shape for aVR: (1, 5000)\n",
      "Lead aVR: y=(829, 998), x=(169, 1043)\n",
      "Grad-CAM shape: (1, 5000)\n",
      "Saved Grad-CAM for aVR\n",
      "Processing Lead: aVL\n",
      "Grad-CAM shape for aVL: (1, 5000)\n",
      "Lead aVL: y=(1007, 1148), x=(169, 1043)\n",
      "Grad-CAM shape: (1, 5000)\n",
      "Saved Grad-CAM for aVL\n",
      "Processing Lead: aVF\n",
      "Grad-CAM shape for aVF: (1, 5000)\n",
      "Lead aVF: y=(1158, 1311), x=(169, 1043)\n",
      "Grad-CAM shape: (1, 5000)\n",
      "Saved Grad-CAM for aVF\n",
      "Processing Lead: V1\n",
      "Grad-CAM shape for V1: (1, 5000)\n",
      "Lead V1: y=(428, 566), x=(1065, 1957)\n",
      "Grad-CAM shape: (1, 5000)\n",
      "Saved Grad-CAM for V1\n",
      "Processing Lead: V2\n",
      "Grad-CAM shape for V2: (1, 5000)\n",
      "Lead V2: y=(585, 680), x=(1065, 1957)\n",
      "Grad-CAM shape: (1, 5000)\n",
      "Saved Grad-CAM for V2\n",
      "Processing Lead: V3\n",
      "Grad-CAM shape for V3: (1, 5000)\n",
      "Lead V3: y=(708, 828), x=(1065, 1957)\n",
      "Grad-CAM shape: (1, 5000)\n",
      "Saved Grad-CAM for V3\n",
      "Processing Lead: V4\n",
      "Grad-CAM shape for V4: (1, 5000)\n",
      "Lead V4: y=(829, 998), x=(1065, 1957)\n",
      "Grad-CAM shape: (1, 5000)\n",
      "Saved Grad-CAM for V4\n",
      "Processing Lead: V5\n",
      "Grad-CAM shape for V5: (1, 5000)\n",
      "Lead V5: y=(1007, 1148), x=(1065, 1957)\n",
      "Grad-CAM shape: (1, 5000)\n",
      "Saved Grad-CAM for V5\n",
      "Processing Lead: V6\n",
      "Grad-CAM shape for V6: (1, 5000)\n",
      "Lead V6: y=(1158, 1311), x=(1065, 1957)\n",
      "Grad-CAM shape: (1, 5000)\n",
      "Saved Grad-CAM for V6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fg/skgs0fc51mbfy2l3s70y4qgm0000gn/T/ipykernel_6615/739683345.py:195: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap = get_cmap(\"jet\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad-CAM overlay image saved to ../output/ecg_grad_cam_overlay.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from torch import nn\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from utils_builder import ECGCLIP\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from PIL import Image\n",
    "from matplotlib.cm import get_cmap\n",
    "from scipy.io import loadmat\n",
    "\n",
    "\n",
    "class Args:\n",
    "    ecg_path = \"../zeroshot/10_LV/QMH1001803PECG20231200010936_20231205092900_0.mat\"\n",
    "    ecg_image_path = \"../zeroshot/10_LV/10.jpeg\"\n",
    "    labels = [\"LV Impaired\"]\n",
    "    config_path = \"self_test.yaml\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    target_label = 0\n",
    "    output_path = \"../output/ecg_grad_cam_overlay.png\"\n",
    "    ecg_encoder_path = \"../ckpt/Vit_69_best_encoder.pth\"\n",
    "    text_encoder_path = \"../ckpt/Vit_69_best_ckpt.pth\"\n",
    "\n",
    "\n",
    "class MultiModalClassifier(nn.Module):\n",
    "    def __init__(self, base_model, labels):\n",
    "        super().__init__()\n",
    "        self.model = base_model\n",
    "        self.labels = labels\n",
    "        self.use_cam_mode = False  # 添加标志，默认关闭 CAM 模式\n",
    "\n",
    "    def forward(self, ecg, text=None):\n",
    "        if self.use_cam_mode:  # 如果是 CAM 模式，调用 forward_for_cam\n",
    "            return self.forward_for_cam(ecg)\n",
    "        \n",
    "        print(f\"Text input: {text}\")\n",
    "        \n",
    "        tokenizer_output = self.model._tokenize(text)\n",
    "        input_ids = tokenizer_output['input_ids'].to(ecg.device)\n",
    "        attention_mask = tokenizer_output['attention_mask'].to(ecg.device)\n",
    "        outputs = self.model(ecg=ecg, input_ids=input_ids, attention_mask=attention_mask)\n",
    "        proj_ecg_emb = outputs['proj_ecg_emb'][0]\n",
    "        proj_text_emb = outputs['proj_text_emb'][0]\n",
    "        logits_per_ecg = torch.matmul(proj_ecg_emb, proj_text_emb.T)\n",
    "        probs = logits_per_ecg.softmax(dim=-1)\n",
    "        return probs\n",
    "\n",
    "    def forward_for_cam(self, ecg):\n",
    "        if ecg.dim() == 4:  # 如果输入形状是 [batch_size, channels, 1, length]\n",
    "            print(f\"Input shape before squeeze: {ecg.shape}\")\n",
    "            ecg = ecg.squeeze(2)\n",
    "            print(f\"Input shape after squeeze: {ecg.shape}\")\n",
    "\n",
    "        default_text = [self.labels[0]]  # 假设 labels[0] 对应目标标签\n",
    "        print(f\"Default text input: {default_text}\")\n",
    "        tokenizer_output = self.model._tokenize(default_text)\n",
    "        input_ids = tokenizer_output['input_ids'].to(ecg.device)\n",
    "        attention_mask = tokenizer_output['attention_mask'].to(ecg.device)\n",
    "        outputs = self.model(ecg=ecg, input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        proj_ecg_emb = outputs['ecg_emb']\n",
    "        if isinstance(proj_ecg_emb, list):\n",
    "            proj_ecg_emb = proj_ecg_emb[0]\n",
    "\n",
    "        if proj_ecg_emb.dim() == 2:  # 如果输出是 [B, L]\n",
    "            proj_ecg_emb = proj_ecg_emb.unsqueeze(1)\n",
    "        print(f\"Output shape: {proj_ecg_emb.shape}\")\n",
    "        return proj_ecg_emb\n",
    "\n",
    "\n",
    "def preprocess_ecg(ecg_path):\n",
    "    ecg = loadmat(ecg_path)['ecg_signals']\n",
    "    ecg = ecg.astype(np.float32)\n",
    "\n",
    "    ecg = ecg[:, :5000]\n",
    "    \n",
    "    # normalzie to 0-1\n",
    "    ecg = (ecg - np.min(ecg))/(np.max(ecg) - np.min(ecg) + 1e-8)\n",
    "    \n",
    "    ecg = torch.from_numpy(ecg).float()\n",
    "\n",
    "    ecg = ecg.unsqueeze(0)\n",
    "    print(f\"ECG signal shape: {ecg.shape}\")\n",
    "    return ecg\n",
    "\n",
    "\n",
    "def reshape_transform_ecg(tensor):\n",
    "    if tensor.dim() == 3:\n",
    "        tensor = tensor.unsqueeze(2)\n",
    "    print(f\"Reshaped tensor shape: {tensor.shape}\")\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def load_yaml_config(config_path):\n",
    "    try:\n",
    "        with open(config_path, \"r\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        return config\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"YAML config file not found: {config_path}\")\n",
    "    except yaml.YAMLError as e:\n",
    "        raise ValueError(f\"Error parsing YAML config file: {e}\")\n",
    "\n",
    "\n",
    "def load_pretrained_model(config_path, ecg_encoder_path, text_encoder_path, device):\n",
    "    config = load_yaml_config(config_path)\n",
    "    model = ECGCLIP(config['network'])\n",
    "    print(config['network'])\n",
    "    ecg_weights = torch.load(ecg_encoder_path, map_location=device)\n",
    "    model.ecg_encoder.load_state_dict(ecg_weights, strict=False)\n",
    "    text_weights = torch.load(text_encoder_path, map_location=device)\n",
    "    model.lm_model.load_state_dict(text_weights, strict=False)\n",
    "    return model.to(device).eval()\n",
    "\n",
    "\n",
    "def overlay_grad_cam_on_image(ecg_image_path, grad_cams, output_path, alpha=0.5):\n",
    "    \"\"\"\n",
    "    将 Grad-CAM 热图叠加到 ECG 图像上，每条导联对应特定的图像区域。\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    from matplotlib.cm import get_cmap\n",
    "\n",
    "    # 1. 加载原始 ECG 图像\n",
    "    ecg_image = Image.open(ecg_image_path).convert(\"RGB\")\n",
    "    ecg_image_np = np.array(ecg_image)\n",
    "\n",
    "    # 2. 定义每条导联的坐标区域（基于更正后的坐标）\n",
    "    lead_coords = {\n",
    "        \"I\": (680, 900, 270, 1660),\n",
    "        \"II\": (930, 1080, 270, 1660),\n",
    "        \"III\": (1125, 1316, 270, 1660),\n",
    "        \"aVR\": (1317, 1585, 270, 1660),\n",
    "        \"aVL\": (1600, 1824, 270, 1660),\n",
    "        \"aVF\": (1840, 2082, 270, 1660),\n",
    "        \"V1\": (680, 900, 1695, 3115),\n",
    "        \"V2\": (930, 1080, 1695, 3115),\n",
    "        \"V3\": (1125, 1316, 1695, 3115),\n",
    "        \"V4\": (1317, 1585, 1695, 3115),\n",
    "        \"V5\": (1600, 1824, 1695, 3115),\n",
    "        \"V6\": (1840, 2082, 1695, 3115),\n",
    "    }\n",
    "\n",
    "    # 3. 确保坐标适配图片分辨率\n",
    "    reference_height = 2480  # 假设参考高度\n",
    "    reference_width = 3508   # 假设参考宽度\n",
    "    image_height, image_width, _ = ecg_image_np.shape\n",
    "\n",
    "    # 根据实际图片大小调整坐标比例\n",
    "    height_ratio = image_height / reference_height\n",
    "    width_ratio = image_width / reference_width\n",
    "\n",
    "    lead_coords_scaled = {\n",
    "        lead: (\n",
    "            int(y_start * height_ratio),\n",
    "            int(y_end * height_ratio),\n",
    "            int(x_start * width_ratio),\n",
    "            int(x_end * width_ratio)\n",
    "        )\n",
    "        for lead, (y_start, y_end, x_start, x_end) in lead_coords.items()\n",
    "    }\n",
    "\n",
    "    print(f\"ECG image resolution: {image_height} x {image_width}\")\n",
    "    print(f\"Scaled lead coordinates: {lead_coords_scaled}\")\n",
    "\n",
    "    # 4. 结果图像\n",
    "    blended_image = ecg_image_np.copy()\n",
    "\n",
    "    # 5. 遍历每条导联的 Grad-CAM\n",
    "    lead_names = list(lead_coords.keys())  # 确保顺序与 grad_cams 一致\n",
    "    for lead_idx, grad_cam in enumerate(grad_cams):\n",
    "        lead_name = lead_names[lead_idx]\n",
    "\n",
    "        print(f\"Processing Lead: {lead_name}\")\n",
    "        print(f\"Grad-CAM shape for {lead_name}: {grad_cam.shape}\")\n",
    "\n",
    "        y_start, y_end, x_start, x_end = lead_coords_scaled[lead_name]\n",
    "        print(f\"Lead {lead_name}: y=({y_start}, {y_end}), x=({x_start}, {x_end})\")\n",
    "        print(f\"Grad-CAM shape: {grad_cam.shape}\")\n",
    "\n",
    "        # Grad-CAM 归一化到 [0, 1]\n",
    "        grad_cam_normalized = (grad_cam - grad_cam.min()) / (grad_cam.max() - grad_cam.min())\n",
    "        grad_cam_resized = Image.fromarray((grad_cam_normalized * 255).astype(np.uint8))\n",
    "\n",
    "        grad_cam_resized.save(f\"grad_cam_{lead_name}.png\")\n",
    "        print(f\"Saved Grad-CAM for {lead_name}\")\n",
    "\n",
    "        grad_cam_resized = grad_cam_resized.resize((x_end - x_start, y_end - y_start), resample=Image.BICUBIC)\n",
    "        grad_cam_resized_np = np.array(grad_cam_resized)\n",
    "\n",
    "        # 转换为彩色热图\n",
    "        cmap = get_cmap(\"jet\")\n",
    "        grad_cam_colored = cmap(grad_cam_resized_np / 255.0)[:, :, :3]  # 去掉 alpha 通道\n",
    "        grad_cam_colored = (grad_cam_colored * 255).astype(np.uint8)\n",
    "\n",
    "        # 叠加到对应导联区域\n",
    "        blended_image[y_start:y_end, x_start:x_end, :] = (\n",
    "            (1 - alpha) * blended_image[y_start:y_end, x_start:x_end, :]\n",
    "            + alpha * grad_cam_colored\n",
    "        )\n",
    "\n",
    "    # 6. 保存并显示结果\n",
    "    blended_image_pil = Image.fromarray(blended_image.astype(np.uint8))\n",
    "    blended_image_pil.save(output_path)\n",
    "    blended_image_pil.show()\n",
    "\n",
    "    print(f\"Grad-CAM overlay image saved to {output_path}\")\n",
    "\n",
    "\n",
    "def run_ecg_grad_cam(args):\n",
    "\n",
    "    # load model\n",
    "    model = load_pretrained_model(args.config_path, args.ecg_encoder_path, args.text_encoder_path, args.device)\n",
    "\n",
    "    classifier = MultiModalClassifier(model, args.labels).to(args.device).eval()\n",
    "    last_block = getattr(model.ecg_encoder, f'block{model.ecg_encoder.depth - 1}')\n",
    "    target_layers = [last_block.attn]\n",
    "    ecg_signal = preprocess_ecg(args.ecg_path).to(args.device)\n",
    "\n",
    "    ecg_signal = ecg_signal.unsqueeze(2)\n",
    "    classifier.use_cam_mode = True\n",
    "\n",
    "    print(f\"ECG signal shape before Grad-CAM: {ecg_signal.shape}\")\n",
    "    grad_cams = []  # 存储每个导联的 Grad-CAM 热图\n",
    "    for lead_idx in range(12):  # 针对每个导联单独计算\n",
    "        print(f\"Processing Lead {lead_idx + 1}\")\n",
    "\n",
    "        # 提取单导联信号\n",
    "        single_lead_signal = ecg_signal[:, lead_idx:lead_idx+1, :]  # [1, 1, 5000]\n",
    "\n",
    "        # 创建一个形状为 [1, 12, 5000] 的输入信号，其中其他导联填充为 0\n",
    "        input_signal = torch.zeros_like(ecg_signal)  # [1, 12, 5000]\n",
    "        input_signal[:, lead_idx, :] = single_lead_signal[:, 0, :]  # 替换目标导联的信号\n",
    "\n",
    "        print(f\"Input signal shape for Lead {lead_idx + 1}: {input_signal.shape}\")\n",
    "\n",
    "        # Initialize Grad-CAM for this lead\n",
    "        cam = GradCAM(model=classifier, target_layers=target_layers, reshape_transform=reshape_transform_ecg)\n",
    "        targets = [ClassifierOutputTarget(args.target_label)]\n",
    "\n",
    "        # Compute Grad-CAM for this lead\n",
    "        grayscale_cam = cam(input_tensor=input_signal, targets=targets)[0]  # 输出形状为 [5000]\n",
    "        grad_cams.append(grayscale_cam)\n",
    "\n",
    "    print(f\"Generated Grad-CAM for {len(grad_cams)} leads.\")\n",
    "\n",
    "    # 将每个 Grad-CAM 热图叠加到 ECG 图像\n",
    "    overlay_grad_cam_on_image(args.ecg_image_path, grad_cams, args.output_path, alpha=0.5)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = Args()\n",
    "    run_ecg_grad_cam(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
